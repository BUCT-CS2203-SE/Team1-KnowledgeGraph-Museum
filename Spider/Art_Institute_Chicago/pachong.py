import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
import argparse
import os

# ======================== å…¨å±€é…ç½®å‚æ•° ========================

# è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼Œé¿å…è¢«ç½‘ç«™æ‹’ç»
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

# é»˜è®¤ä¿å­˜æ–‡ä»¶è·¯å¾„
DEFAULT_OUTPUT_PATH = "data/artic_chinese_artifacts.csv"

# é»˜è®¤æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆæ¯é¡µæœ€å¤š100æ¡æ•°æ®ï¼‰
DEFAULT_MAX_PAGES = 10


# ======================== åŠŸèƒ½å‡½æ•°å®šä¹‰ ========================

def get_artworks_from_api(page=1, limit=100):
    """
    ä» AIC çš„ API è·å–ä¸ "China" å…³é”®è¯ç›¸å…³çš„æ–‡ç‰©åˆ—è¡¨ï¼ˆä¸åŒ…å«è¯¦æƒ…ï¼‰ã€‚
    :param page: é¡µç ï¼Œä»1å¼€å§‹
    :param limit: æ¯é¡µæœ€å¤šè·å–çš„æ–‡ç‰©æ•°ï¼ˆæœ€å¤§100ï¼‰
    :return: è¿”å›æ–‡ç‰©å­—å…¸åˆ—è¡¨
    """
    url = "https://api.artic.edu/api/v1/artworks/search"
    params = {
        "q": "China",
        "page": page,
        "limit": limit,
        "fields": "id,title,date_display,image_id"  # åªè¯·æ±‚éœ€è¦çš„å­—æ®µ
    }
    response = requests.get(url, params=params)
    if response.status_code != 200:
        return []
    return response.json().get("data", [])


def construct_detail_url(art_id):
    """
    æ ¹æ®æ–‡ç‰©IDæ„é€ è¯¦æƒ…é¡µé“¾æ¥
    """
    return f"https://www.artic.edu/artworks/{art_id}"


def construct_image_url(image_id):
    """
    æ ¹æ® image_id æ„é€ å›¾åƒ URLï¼Œè¿”å›é«˜æ¸…å›¾é“¾æ¥
    """
    if image_id:
        return f"https://www.artic.edu/iiif/2/{image_id}/full/843,/0/default.jpg"
    return ""


def get_description_from_page(detail_url):
    """
    è®¿é—®è¯¦æƒ…é¡µå¹¶ä» HTML ä¸­æå–æ–‡ç‰©æè¿°ï¼ˆå¦‚æœ‰ï¼‰
    """
    try:
        response = requests.get(detail_url, headers=HEADERS)
        soup = BeautifulSoup(response.text, "html.parser")
        desc_tag = soup.select_one(".m-article-text")  # CSS é€‰æ‹©å™¨æ‰¾ç®€ä»‹æ®µè½
        if desc_tag:
            return desc_tag.get_text(strip=True)
    except Exception as e:
        print(f"â—ï¸è·å–è¯¦æƒ…å¤±è´¥: {detail_url} é”™è¯¯: {e}")
    return ""


def crawl_all_artworks(max_pages=5):
    """
    ä¸»çˆ¬è™«é€»è¾‘ï¼šå¾ªç¯åˆ†é¡µè¯·æ±‚ APIï¼Œå¹¶æŠ“å–è¯¦æƒ…é¡µæè¿°ã€‚
    :param max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
    :return: è¿”å›å…¨éƒ¨æ–‡ç‰©è®°å½•çš„åˆ—è¡¨ï¼ˆæ¯æ¡ä¸ºä¸€ä¸ªå­—å…¸ï¼‰
    """
    all_data = []
    for page in range(1, max_pages + 1):
        print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ")
        artworks = get_artworks_from_api(page)
        if not artworks:
            break  # å¦‚æœä¸ºç©ºï¼Œè¯´æ˜æ²¡å†…å®¹äº†
        for art in artworks:
            detail_url = construct_detail_url(art["id"])
            description = get_description_from_page(detail_url)
            record = {
                "title": art.get("title", ""),
                "time": art.get("date_display", ""),
                "description": description,
                "detail_url": detail_url,
                "image_url": construct_image_url(art.get("image_id"))
            }
            all_data.append(record)
            print(f"  -> æŠ“å–æˆåŠŸ: {record['title'][:30]}...")
            time.sleep(0.5)  # åŠ ä¸€ç‚¹å»¶æ—¶ï¼Œé¿å…è¯·æ±‚å¤ªå¿«è¢«å°
    return all_data


def save_to_csv(data, filename):
    """
    å°†æ•°æ®ä¿å­˜ä¸º UTF-8 ç¼–ç çš„ CSV æ–‡ä»¶
    :param data: æ–‡ç‰©æ•°æ®åˆ—è¡¨ï¼ˆåˆ—è¡¨å…ƒç´ æ˜¯å­—å…¸ï¼‰
    :param filename: ä¿å­˜è·¯å¾„
    """
    # è‡ªåŠ¨åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"âœ… æ•°æ®ä¿å­˜è‡³ï¼š{filename}")


# ======================== å‘½ä»¤è¡Œä¸»å…¥å£ ========================

def main():
    """
    å‘½ä»¤è¡Œå…¥å£å‡½æ•°ï¼Œæ”¯æŒç”¨æˆ·ä¼ å…¥å‚æ•°æ§åˆ¶ä¿å­˜è·¯å¾„å’Œçˆ¬å–é¡µæ•°
    """
    parser = argparse.ArgumentParser(description="çˆ¬å–èŠåŠ å“¥è‰ºæœ¯åšç‰©é¦†ä¸­ä¸ä¸­å›½ç›¸å…³çš„æ–‡ç‰©ä¿¡æ¯")

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„å‚æ•°
    parser.add_argument(
        "--output",
        type=str,
        default=DEFAULT_OUTPUT_PATH,
        help=f"ä¿å­˜çš„ CSV è·¯å¾„ï¼ˆé»˜è®¤: {DEFAULT_OUTPUT_PATH}ï¼‰"
    )

    # æœ€å¤§é¡µæ•°å‚æ•°
    parser.add_argument(
        "--pages",
        type=int,
        default=DEFAULT_MAX_PAGES,
        help=f"æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤: {DEFAULT_MAX_PAGES}ï¼‰"
    )

    # è§£æå‚æ•°
    args = parser.parse_args()

    # æ‰§è¡Œçˆ¬è™«å¹¶ä¿å­˜ç»“æœ
    data = crawl_all_artworks(max_pages=args.pages)
    save_to_csv(data, filename=args.output)


# å½“æ­¤è„šæœ¬è¢«ç›´æ¥è¿è¡Œæ—¶ï¼Œæ‰§è¡Œ main()
if __name__ == "__main__":
    main()
