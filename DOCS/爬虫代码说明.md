
### 史密斯学会

`Spider/Smithsonian/`

---

### 旧金山亚洲艺术博物馆

`Spider/Asian_Art_Museum`

---

### 芝加哥艺术博物馆

`Spider/Art_Institute_Chicago`

该脚本（pachong.py）用于从芝加哥艺术博物馆（Art Institute of Chicago, AIC）的 API 爬取与中国相关的文物信息，并将结果保存为 CSV 文件。文物信息包括文物的标题、创建时间、描述、详情页链接及图片链接。

#### 功能简介：

1. **爬取文物信息**：从 AIC API 获取包含“China”关键词的文物列表。
2. **抓取文物描述**：访问每个文物的详情页并提取描述信息。
3. **保存为 CSV**：将抓取到的文物数据保存为 UTF-8 编码的 CSV 文件。

------

### 主要模块与函数说明

#### 1. **全局配置参数**

```python
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

DEFAULT_OUTPUT_PATH = "data/artic_chinese_artifacts.csv"
DEFAULT_MAX_PAGES = 10
```

- **HEADERS**：请求头，模拟浏览器访问，避免被网站封禁。
- **DEFAULT_OUTPUT_PATH**：默认保存文件路径，存储爬取的文物数据。
- **DEFAULT_MAX_PAGES**：默认最大爬取页数，每页最多获取100条数据。

#### 2. **功能函数**

##### 2.1 `get_artworks_from_api(page=1, limit=100)`

该函数通过发送 HTTP 请求从 AIC 的 API 获取与“China”相关的文物列表（不包含详细信息）。

- **参数**：
  - `page`: 页码，从1开始。
  - `limit`: 每页最多获取的文物数，最大为100条。
- **返回**：返回文物字典列表，每个字典包含文物的基础信息，如：`id`、`title`、`date_display`、`image_id`。

##### 2.2 `construct_detail_url(art_id)`

根据文物的 ID 构造其详情页 URL。

- **参数**：
  - `art_id`: 文物的 ID。
- **返回**：返回详情页的完整 URL。

##### 2.3 `construct_image_url(image_id)`

根据图像的 `image_id` 构造高清图像的 URL。

- **参数**：
  - `image_id`: 图像的 ID。
- **返回**：高清图像的 URL。

##### 2.4 `get_description_from_page(detail_url)`

访问文物的详情页，并从 HTML 中提取文物的描述信息（如果存在）。

- **参数**：
  - `detail_url`: 文物详情页的 URL。
- **返回**：返回文物描述文本。如果没有描述，则返回空字符串。

##### 2.5 `crawl_all_artworks(max_pages=5)`

主爬虫逻辑。该函数会抓取多个页面的数据，每个页面最多包含 `limit` 条文物数据。对于每个文物，还会抓取其详细描述信息。

- **参数**：
  - `max_pages`: 最大爬取页数。
- **返回**：返回包含所有文物信息的列表，每条信息是一个字典，包含文物的标题、时间、描述、详情页链接和图像 URL。

##### 2.6 `save_to_csv(data, filename)`

将抓取的文物数据保存为 CSV 文件。

- **参数**：
  - `data`: 包含所有文物数据的列表，每个元素是一个字典。
  - `filename`: 输出文件路径，保存为 CSV 文件。
- **返回**：无返回值，数据将保存到指定路径的 CSV 文件中。

------

### 3. **命令行参数说明**

#### 3.1 `--output`

- **功能**：指定输出 CSV 文件的路径。
- **默认值**：`data/artic_chinese_artifacts.csv`。
- **示例**：`--output "output/artifacts.csv"`

#### 3.2 `--pages`

- **功能**：指定要爬取的最大页数。
- **默认值**：`10`。
- **示例**：`--pages 5`（表示爬取 5 页数据）

------

### 4. **命令行执行**

该脚本支持通过命令行参数来配置爬虫的输出路径和最大爬取页数。例如，要爬取 3 页文物数据并保存为指定文件，可以执行以下命令：

```
python pachong.py --output "data/artifacts.csv" --pages 3
```

------

### 5. **注意事项**

- **访问限制**：该脚本通过模拟浏览器请求来爬取数据，但请注意不要频繁访问网站，以避免被封禁。使用 `time.sleep(0.5)` 来降低请求频率。
- **数据来源**：数据来自芝加哥艺术博物馆的公开 API，确保在遵守 API 使用条款的情况下使用该脚本。
- **错误处理**：如果访问详情页失败或网络出现问题，爬虫会记录错误并继续抓取其他文物。

------

### 6. **示例输出**

在命令行执行脚本后，会将爬取到的文物数据保存为 CSV 文件。每条记录包括以下字段：

- `title`：文物标题
- `time`：文物创建时间
- `description`：文物描述（如有）
- `detail_url`：文物详情页链接
- `image_url`：文物图像链接

例如，CSV 文件内容可能如下所示：

```cvs
title,time,description,detail_url,image_url
"Tree Peonies in Full Bloom",1911,"This boldly decorative painting is a significant document of Chinese-American relations...",https://www.artic.edu/artworks/25247,https://www.artic.edu/iiif/2/d2b56c9c-0824-9d66-fab6-2b02493f6855/full/843,/0/default.jpg
```

------

### 7. **依赖项**

- `requests`：用于发送 HTTP 请求。
- `pandas`：用于处理和保存数据为 CSV 文件。
- `BeautifulSoup`：用于解析和提取 HTML 内容。
- `argparse`：用于解析命令行参数。
- `os`：用于处理文件路径和目录。

